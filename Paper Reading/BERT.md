# 背景   
1. 在计算机视觉领域：   
- 在计算机视觉领域的发展早期，研究人员发现，可以先在一个大型的数据集（比如 ImageNet）上训练一个卷积神经网络（CNN）模型。训练完成后，这个模型就能学到很多通用的图像特征（比如边缘、形状、纹理等），这些特征对各种视觉任务都是有用的。

- 然后，我们就可以把这个已经训练好的模型迁移到其他计算机视觉任务上（比如图像分类、物体检测、图像分割等），通过微调（fine-tuning）或者作为特征提取器来用，从而提升这些任务的性能，即让它们表现得更好、识别更准。

- 这就是“迁移学习（Transfer Learning）”的一个经典做法，也说明了CNN模型具有良好的通用性。
2. 在自然语言处理领域：
- 但是在 BERT 出现之前，NLP 领域并没有一个像 CNN + ImageNet 那样的统一框架——也就是说，没有一种预训练的深度模型可以很好地迁移到各种 NLP 任务（如问答、情感分析、命名实体识别等）中。

- 因为缺少统一的预训练模型，NLP 的研究者或者工程师们往往要为每个具体任务单独设计神经网络结构并从头训练，这既费时又不容易取得很好的效果。

- BERT 可以在大规模语料上预训练，然后迁移到不同的 NLP 任务中，取得很好的效果，像“CV 里的 CNN+ImageNet”那样，在 NLP 里实现了“预训练 + 微调”的范式。

- BERT 不仅让模型训练过程更简单（不用从零设计架构），还能让模型效果更好。

**BERT**也是站在巨人的肩膀上的。

# 摘要

**BERT**，它的全称是 Bidirectional Encoder Representations from Transformers，即“来自 Transformer 的双向编码器表示”。

与当时的一些语言表示模型（如 Peters 的 ELMo 和 Radford 的 GPT）不同，BERT 的设计目标是在所有网络层中同时考虑句子的左侧上下文和右侧上下文，来预训练深层的双向语言表示。而早期模型大多是单向的（只能看左边或右边）。

> 这里来解释一下为什么不同：
**ELMo（Embeddings from Language Models，2018）**：
    - 特点：双向 LSTM
    - 原理： ELMo 使用一个前向（left-to-right）LSTM 和一个后向（right-to-left）LSTM，分别读取句子。
    - 问题： 虽然它是“前向 + 后向”，但这两个方向是分开训练的，不是“同时考虑左右”。
    - 层级处理： 它的双向性不是在每一层都共同作用，而是最后将两个方向的输出拼接起来。
所以，ELMo 是“浅层双向”而不是 BERT 那种“深层双向”。
**GPT（Generative Pre-trained Transformer，2018）**：
    - 特点：单向 Transformer（只看左边）
    - 原理： GPT 使用标准的 Transformer decoder 架构，在训练时只看每个词的左边上下文（即过去的词）。
    - 限制： 模型在预测一个词时，不能看到右边，否则就不是“自回归语言建模”了。
所以 GPT 是“严格单向”，只有“从左到右”的信息流。
**BERT（Bidirectional Encoder Representations from Transformers，2018）**：
    - 特点：真正的“深层双向”Transformer
    - 原理： BERT 使用 Transformer encoder，训练时通过 Masked Language Modeling（MLM），随机遮掉句中的部分词，然后让模型根据上下文去预测它。
    - 优势： 因为每个被遮住的词周围的词都可以来自左边和右边，所以 BERT 在每一层都同时考虑左、右上下文。
BERT 是“jointly conditioning on both left and right context in all layers”，这是它和 ELMo/GPT 最大的区别。
> > 这里我对这个优势的理解是：它能够在模型的每一层中，同时利用一个词的左边和右边的信息（上下文）来理解这个词的含义。


因此，BERT 预训练模型只需要在其上面添加一个任务相关的输出层，然后进行微调（fine-tuning），就可以在各种 NLP 任务（如问答、语言推理等）上获得非常好的效果，而不需要对整个模型结构做复杂的任务专属改动。

BERT 的概念上很简单，但效果上非常强大。BERT 在 11个自然语言处理任务上都刷新了最新的最优性能（state-of-the-art），然后便是一些BERT的战绩：
- GLUE 总得分提升至 80.5%，比之前最好结果提升了 7.7 个百分点。
- MultiNLI 准确率提升到 86.7%，提升了 4.6 个百分点。
- SQuAD v1.1（问答任务）F1 提升到 93.2，提升 1.5 个百分点。
- SQuAD v2.0 测试集 F1 提升到 83.1，提升了 5.1 个百分点。

摘要总结：
	1.	BERT 是一个基于 Transformer 的双向语言表示模型。
	2.	它可以在不带标签的文本上预训练，然后通过微调迁移到各种任务。
	3.	不同于早期只考虑单向上下文的模型，BERT 从一开始就使用了双向上下文。
	4.	不需要设计任务专属结构，只需加一个输出层就能适配各种任务。
	5.	它在一系列主流 NLP 任务中刷新了 SOTA（最优性能），表明它效果强大。

# 1 引言

引言上来先说明语言模型的预训练已经被证明能够显著提升很多自然语言处理任务的性能。
括号里提到的是几篇相关的重要论文，包括：
- Dai & Le (2015)：比较早期的预训练思想，提出在序列建模中使用未标注数据预训练。
- Peters et al., 2018a：就是提出 ELMo 的论文。
- Radford et al., 2018：就是第一版 GPT。
- Howard and Ruder, 2018：提出了 ULMFiT，一种迁移学习方法。        

> 这些工作都在推动一个共同方向：先用大量无监督文本预训练模型，再用于下游任务微调。

这些任务包括了“句子级别”的任务，比如：
- 自然语言推理（NLI）：判断两个句子之间的逻辑关系（比如：蕴含、矛盾、中立）。
  - Bowman et al., 2015：SNLI 数据集。
  - Williams et al., 2018：MultiNLI 数据集。
- 句子复述识别（Paraphrasing）：判断两个句子是否表达相同含义。引用的是微软的著名 MSRP 数据集（Dolan and Brockett, 2005）。

> 这些任务的目标是通过“理解整体”来判断句子之间的关系。

除了句子级别的任务，还有 “词级别”任务，比如：
- 命名实体识别（NER）：识别句子中哪些词是人名、地名、机构名等。
  - 对应数据集：CoNLL-2003（Tjong Kim Sang & De Meulder）
- 问答（QA）：例如在 SQuAD 任务中，模型需要在段落中找出正确的答案位置。
  - 对应数据集：SQuAD（Rajpurkar et al., 2016）

> 这些任务的特点是：模型要对每一个词进行精细判断，输出粒度非常细。

现有的将预训练语言表示应用于下游任务的方法主要有两种：
- 基于特征的方法（feature-based）
- 微调的方法（fine-tuning）

Feature-based（以 ELMo 为代表）：基于特征的方法是将预训练模型作为一个特征提取器（feature extractor）来使用。也就是说，我们使用预训练语言模型（如 ELMo）对文本进行编码，提取出语义表示向量（embedding），然后把这些向量输入到我们自己设计的任务模型中（比如一个分类器、一个序列标注模型等），进行下游任务。
关键点：
- 预训练模型的参数不更新，只用它提取表示。
- 下游任务的模型（比如分类器）会被单独训练。

Fine-tuning（以 GPT 为代表）：微调的方法是将预训练模型和下游任务模型联合训练。即，我们在加载预训练语言模型之后，会加入一个任务特定的输出层（例如分类器），然后对整个模型（包括预训练部分）一起进行训练，通过在下游任务数据上的梯度反向传播来微调预训练模型的参数。
关键点：
- 预训练模型的参数会被更新。
- 下游模型只是预训练模型上增加的一小层，通常是 task-specific 的输出层。

两种方法的共同点：这两种方法（ELMo 和 GPT）虽然应用方式不同，但它们在预训练阶段有一个共同点：
- 都使用 单向语言模型（unidirectional language models） 来学习通用的语言表示。
  - ELMo 是前向 + 后向 LSTM（但独立）
  - GPT 是只看左边的 Transformer（单向）

**这里引出了BERT的不同** ：也就是说，它们虽然都能从大语料中学到一些通用知识，但它们的“看世界方式”是单向的，不如 BERT 那样“左右都看”。

作者认为，当前的方法限制了预训练语言表示的潜力，尤其是对于 **微调型方法（fine-tuning）** 来说更是如此，它们都没能充分发挥预训练模型的表达能力。
一个主要的限制是：标准语言模型是“单向”的（只能从左到右，或从右到左）。
1. 这种限制限制了在预训练阶段可以使用的模型结构的选择。大多数语言模型（像 GPT）只能看句子的“过去”——比如预测一个词只能看到左边的词，而不能看到右边的词。这种设计对某些任务效果不好。
2. 模型不能完整地看到预测词的上下文，这样对理解语义是有限的。这种单向的限制，对于“句子级任务”来说表现可能不理想；而在一些“词级任务”中（例如问答），这种限制可能非常有害，因为这类任务必须同时利用左边和右边的上下文信息。

接下来，作者开始从动机、关键设计、贡献来对BERT进行总结。
BERT：是一种改进了传统微调方法的语言预训练模型，使用“双向 Transformer 编码器表示”方法。
1. 避免单向性的限制（核心创新）：使用“掩码语言模型（Masked Language Model, MLM）”来克服传统模型“只能单向理解”的局限。
    - 以前的模型（如 GPT）只能从左往右读；
    - BERT 则通过 MLM 可以在训练时同时看到一个词的左边和右边。
> MLM 受启发于 Cloze 任务:”…inspired by the Cloze task (Taylor, 1953).”
“填空题（Cloze test）”：
&emsp;给你一句话，遮掉某个词，让你根据上下文来猜这个词。
&emsp;这就是 BERT 的预训练核心目标。

2.  MLM 是怎么工作的？
    - 输入中随机遮住一些词（比如用 [MASK] 替代），
    - 模型根据它前后的词，预测原来的词是什么。

> 这个训练方式让模型能同时“看左边”和“看右边”，实现真正的双向建模。

3. 与 GPT 的单向训练对比:
    - GPT：只能用左边上下文；
    - BERT：通过 MLM，可以融合左右两边的上下文，训练出深层双向的 Transformer 表示。

4. 另一项预训练任务NSP,除了 MLM，BERT 还加入了另一项训练任务：“下一句预测”（Next Sentence Prediction, NSP），用于学习句子对之间的关系。
    - 句子 A 是不是紧接着句子 B？
    - 适用于问答、句子推理等任务。

总结贡献：
- 验证双向预训练的重要性：对比 GPT（单向）和 ELMo（浅层拼接双向），BERT 通过 MLM 实现了真正的深层双向训练，是模型具备了更强的表达能力，更适合复杂语言理解任务。
- 减少人工设计结构的需求：不需要为每个任务专门设计模型结构，只要在训练好的模型后加一层输出头部就可以直接迁移，而且效果比很多复杂的“手工设计”模型还好！
- 刷新了多个 NLP 任务的性能纪录：无论是句子级任务（如自然语言推理、语义匹配），还是词级任务（如命名实体识别、问答），所有任务都达到了当时最好的结果。
- 开源万岁：[代码和预训练模型](https://github.com/google-research/bert)

# 2 相关工作

本节简要回顾了 **pre-training general language representations（预训练通用语言表示模型）** 最常用的方法。

## 2.1 Unsupervised Feature-based Approaches

本小节重点介绍了 BERT 出现之前已有的 **无监督特征提取型预训练方法（feature-based approaches）** 的 **Related Work（相关工作）**。

1. 首先是词表示的演变：
长期以来，自然语言处理（NLP）领域一直在研究如何学习通用的词表示（word representations）。
这些方法包括：
    - 非神经网络方法：如 Brown clustering（Brown et al., 1992）
    - 神经网络方法：如：
        - Word2Vec（Mikolov et al., 2013）
        - GloVe（Pennington et al., 2014）

这些模型产生了“词嵌入向量（word embeddings）”，是现代 NLP 系统的重要组成部分，能比从零训练的表示效果好很多（Turian et al., 2010）。