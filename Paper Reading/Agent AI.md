Agent AI: Surveying the Horizons of Multimodal Interaction
> 这是一篇综述

# 1.引言 

## 1.1动机 

&emsp;使用 LLM 和 VLM 作为智能体代理有巨大潜力，重点关注具有语言能力、视觉认知、上下文记忆、直觉推理和适应能力的模型。利用 LLM 和 VLM 作为代理，特别是在游戏、机器人和医疗保健等领域，不仅为最先进的 AI 系统提供了一个严格的评估平台，而且预示着以代理为中心的 AI 将对社会和行业产生的变革性影响.
>  LLM (Large Language Model)： LLM是一种通过大规模文本数据训练的语言模型，能够生成、理解和处理自然语言。它们基于深度神经网络（如Transformers），通过大量的语料库学习语言的结构、上下文、语法等。典型的LLM包括GPT系列、BERT等。它们的任务包括文本生成、问答、翻译等。

> VLM (Vision-Language Model)： VLM是一种跨模态的模型，结合了视觉和语言信息。这类模型不仅能处理图像，还能将图像与文本信息结合，进行跨模态理解和生成。例如，VLM可以理解图像中的内容并生成与之相关的文本，或根据文本生成对应的图像。像CLIP和BLIP就是此类模型，它们在图像识别、描述生成、图像-文本匹配等任务中表现出色。

## 1.2 背景

LLM->具身智能->交互 

## 1.3 概述 

&emsp;本文重点介绍了 MAA 的一些代表性研究领域，即多模态、游戏（VR/AR/MR）、机器人和医疗保健，旨在提供这些领域中讨论的共同问题方面的全面知识。因此，我们希望学习 MAA 的基本原理，并获得进一步推进其研究的见解。
> Multimodal Agent AI (MAA):多模态智能体, 是一类系统，它们基于对多模态感官输入的理解，在给定环境中生成有效的行动. 

从以下几个角度出发:
* MAA 概述：深入探讨其原理及其在当代应用中的作用，为研究人员提供对其重要性和用途的全面理解。
* 方法论：通过游戏、机器人和医疗保健领域的案例研究，详细说明了 LLM 和 VLM 如何增强 MAA.
* 性能评估：关于使用相关数据集评估 MAA 的指南，重点关注其有效性和泛化能力.
* 伦理考量：探讨具身智能部署对社会的影响和伦理领导地位，强调负责任的开发实践.
* 新兴趋势与未来排行榜：对每个领域的最新发展进行分类，并讨论未来的方向.

本文行文:
* 第二部分概述了具身智能如何从与相关新兴技术的集成中受益，特别是大型基础模型.
* 第三部分描述了我们为训练具身智能提出的新范式和框架.
* 第四部分概述了广泛用于训练具身智能的方法.
* 第五部分对各种类型的代理进行了分类和讨论.
* 第六部分介绍了具身智能在游戏、机器人和医疗保健中的应用 
* 第七部分探讨了研究界为开发一种通用的具身智能所做的工作，这种人工智能能够应用于各种模态、领域，并弥合仿真到现实的差距 
* 第八部分讨论了具身智能的潜力，它不仅依赖于预训练的基础模型，而且还通过利用与环境和用户的交互来不断学习和自我改进 
* 第九部分介绍了我们为训练多模态具身智能而设计的新数据集 
* 第十一部分讨论了人工智能代理的伦理考量、局限性和本文对社会的影响这一热门话题 

# 7.跨模态、跨领域和跨现实的智能代理 

## 7.1 跨模态理解的代理 

多模态理解（multi-modal understanding）在创建通用型人工智能代理中的挑战，以及当前多模态系统的工作方式和潜在的发展方向:
1. **多模态理解的挑战**：创建具备视觉、语言和行为能力的通用型人工智能代理是一项重大挑战。因为目前缺乏包含这些多个模态（视觉、语言、行为）的**大规模数据集**。此外，训练数据通常是针对单一模态（例如，只有视觉或只有语言的）的，这限制了AI在跨模态理解上的能力。 
2. **冻结子模块（Frozen Submodules）**：大多数现代多模态系统使用所谓的“冻结子模块”。这些子模块通常是指已经预训练好的模型，例如语言模型（LLM）和视觉编码器。它们分别在不同的数据集上进行训练，之后被“冻结”并用于多模态系统中，而不会在后续训练中进行调整或更新。为了将视觉信息有效地融入语言模型的理解中，会**训练适应性层（adaptation layers）**，这些层的任务是将视觉编码器的输出转换成适合LLM（语言模型）的嵌入空间。 
3. **挑战和未来发展方向**：虽然目前的方法（即使用冻结的LLM和视觉编码器）已经能够在一定程度上工作，但为了进一步推动跨模态理解的进展，这种策略可能需要改变。最近的一项研究（RT-2模型）表明，如果将视觉编码器和语言模型一起微调（jointly tuning），即在同一个训练过程中同时调整两者的参数，那么在机器人学和视觉-语言任务中，AI系统的表现会显著提高。 

## 7.2 跨领域理解的代理 

创建通用智能代理（generalist agents）所面临的一个关键挑战：不同领域的视觉外观差异和行动空间的不同
1. **现有模型的局限性**:人类具有跨多个领域理解和适应的能力,但是现有的大型语言模型（LLMs）和视觉语言模型（VLMs）往往在训练数据和实际应用的领域之间存在显著差异。当这些模型应用到新的领域时，它们的表现可能会受到影响，因为它们在特定领域之外的数据不够丰富. 
2. **训练代理模型的挑战**：训练智能代理模型来预测特定动作是一个巨大挑战，尤其是当试图开发一个能够跨多个领域有效学习控制系统的单一策略时。不同领域的动作和控制系统差异较大，使得设计一个通用的策略变得复杂。
3. **目前的主流方法**：现代的做法通常是从一个预训练的基础模型开始，然后针对每个特定领域微调（finetune）一个独立的模型。虽然这种方法能够在特定领域内表现较好，但它没有捕捉到各领域之间的共同性，也没有利用跨领域的数据集进行训练，从而导致每个领域的训练数据集较小，无法充分利用多个领域的数据资源。
> 这里我有一个大胆的想法,现在很多无人机的操纵大体是相同的,但是软件和硬件的驱动层面有着各式各样的差异,我们能不能制定一个标准之类的能够让各种代理软件屏蔽掉无人机软件和硬件层面的差异,打造一个适用范围很广的测试无人机各种指标性能算法等的一个平台.

## 7.3 跨模态和跨现实的互动代理 

1. **跨现实理解的挑战**：开发能够成功理解并执行任务的AI代理，在不同现实中（如现实世界与虚拟现实环境）仍然是一个持续的挑战。尤其是，现实世界和虚拟现实的视觉差异以及各自独立的物理规则，使得代理很难同时理解和适应这两种环境。
2. **图像和场景生成的成功**：尽管如此，最近在图像和场景生成方面（如Huang et al., 2023a的研究）已经取得了一些成功，表明AI在跨现实任务中已经有所进展。
> 完事我看看怎么个事[论文地址](https://arxiv.org/abs/2305.00970),但是这是一篇中科院3区的
3. **Sim to Real问题**：在跨现实的上下文中，**Sim to Real**（从模拟到现实的迁移）是一个特别重要的问题。这个问题指的是，当使用在模拟环境中训练的策略（policy）应用到现实世界数据时，如何有效地将模拟训练的结果转移到现实世界中。这是因为模拟环境与现实世界在物理特性、视觉效果等方面的差异，可能导致模型在真实世界中的表现不佳。

## 7.4 仿真到真实的迁移 

1. **Sim-to-real问题**：具身代理，尤其是基于**强化学习（RL）**策略的代理，通常是在模拟环境中进行训练的。然而，这些模拟环境无法完全复制现实世界的特性，比如扰动、光照、重力等物理属性。因此，在模拟环境中训练的模型往往在实际应用中表现不佳，这个问题被称为“sim-to-real问题”。
2. 解决方案：
&emsp; 1. 领域随机化（Domain Randomization）：这种技术通过在模拟环境中随机变化参数（例如物体外观、传感器噪声、光学特性等），以预测现实世界中的不确定性和变化。例如，在训练抓取技能时，可以通过随机化物体形状，来训练代理适应形状略有不同的物体。
&emsp; 2. 领域适应（Domain Adaptation）：领域适应技术通过使用大量模拟图像和少量真实世界图像来训练模型，弥补模拟与现实世界之间的差距。在实际应用中，常用的无配对图像转换方法（如CycleGAN）能够处理不同领域之间的图像配对难题。针对强化学习和模仿学习，已经出现了增强版的方法，如RL-CycleGAN和RetinaGAN。
&emsp; 3. **改进模拟（Improvement of Simulation）**：为了实现更好的sim-to-real转移，需要更为现实的模拟。系统识别技术可以帮助调整模拟参数，使其更加接近现实环境。同时，使用逼真模拟器（例如光线追踪的模拟器）对于图像增强型强化学习尤为有效。
> 这个好 

3. 尽管有上述技术，sim-to-real问题依然是具身代理研究中的一个中心挑战，相关方法仍在不断发展。理论和实践研究都对推动这些技术的进步至关重要。

# 9.新的数据集 

两个新的数据集:“CuisineWorld”和“VideoAnalytica” 

## 9.1 “CuisineWorld” 多智能体游戏数据集 

“CuisineWorld”是一个类似《胡闹厨房》的文字游戏平台，专用于测试多智能体系统的协作效率。重点在于评估智能体对目标的理解能力以及相互间的协调能力。数据集支持两种模式：集中调度模式和去中心化模式。参与者可以选择一种模式进行测试，并将结果提交到排行榜中。
> 胡闹厨房:多个玩家一起协同昨晚的游戏，重点在于多单位协同

## 9.2 “Audio-Video-Language” Pre-training Dataset 

 VideoAnalytica 是一个音频-视频-语言预训练数据集和基准，用于评估视频语言模型的认知推理能力。VideoAnalytica 侧重于利用视频演示作为辅助手段，以更好地理解长篇教学视频中嵌入的复杂、高级推理。VideoAnalytica 强调**多模态信息**（音频、视频、语言）的整合，并要求模型应用领域知识进行上下文分析和信息解读。

主要任务包括：
1. 视频文本检索：准确从教学视频中检索相关文本，需区分相关和无关信息，要求模型深入理解视频内容并分析演示。为增加任务复杂性，数据集中加入由大语言模型生成的“困难负例”，并通过人工验证确保其有效性和公平性。
2. 视频辅助信息问答：要求模型基于视频中提取的信息回答复杂问题，重点在于分析性推理和对视频演示的深刻理解。