EMBODIEDCITY: A BENCHMARK PLATFORM FOR EMBODIED AGENT IN REAL-WORLD CITY ENVIRONMENT
# 摘要
具身人工智能(EmbodiedAI)强调智能体身体在产生类人行为中的作用。
> 这里我查阅了下，更加具体的说法是:
> > 具身智能(Embodied Artificial Intelligence)是指一种基于物理身体进行感知和行动的智能系统，其通过智能体与环境的交互获取信息、理解问题、做出决策并实现行动，从而产生智能行为和适应性 

当前EmbodiedAI的研究重点:构建能够感知、规划和行动的机器学习模型，从而实现与世界的实时交互.
当前研究的缺陷:大多数工作集中在有限的室内环境，例如房间导航或设备操作，对在开放世界场景中体现智能体探索有限. 
缺陷原因：缺乏高质量的模拟器、基准和数据集.

本文构建了一个在真实城市环境中评估具身智能的基准平台.
所做工作:
1. 首先基于真实城市中真实的建筑物、道路和其他元素构建了一个高度逼真的 3D 仿真环境. 
2. 在该环境中，结合历史收集的数据和仿真算法，对行人和车辆流量进行高保真度仿真. 
3. 提供了一套完整的输入和输出接口，方便具身智能体轻松地将任务需求和当前环境观察作为输入，然后做出决策并获得性能评估. 
4. 评估了一些流行的大型语言模型在不同维度和难度的具身智能能力. 

作用：
1. 它将现有具身智能的能力扩展到更高水平. 
2. 它在现实世界中具有更高的实用价值，可以支持更多通用人工智能的潜在应用. 

项目开源:https://embodied-city.fiblab.net

# 1.引言 
EmbodiedAI脱离了静态数据，有望像真实的人类一样能够从环境中学习并与世界动态交互，被认为是实现通用人工智能(AGI)的必要途径. 
EmbodiedAI的训练与测试与环境密切相关,为了加速训练效率并更方便地测试具身智能体,研究人员通常选择构建一个模拟环境作为现实世界的近似和镜像,同时让具身智能体能够以第一人称视角在该环境中实时获取观察结果,生成动作，并接收反馈. 
现有研究缺陷:
1. 有限的环境和任务 
2. 只考虑了EmbodiedAI的室内场景(包括针对特定物体或房间内简单任务分解的视觉问答任务等) 

这类基准实际上将具身智能体的能力验证限制在一个非常狭窄的范围内,任务难度低,与通用人工智能的差距很大.
本文目标:将具身智能体从室内房间扩展到室外城市,将任务从室内空间扩展到更广阔的城市环境 

当前已有工作与本文工作对比: 
![alt text](/assets/EmbodiedCity/table.caption.2.jpg) 
> 由此不难看出,未来主流的关于EmbodiedAI的模拟肯定会在UE上,同时任务也一定是多样化的,不只是简单的QA,还要包含场景理解,只能规划等等 

# 2.相关工作
自动驾驶仿真平台:
1. CARLA:包含城市街道和配备传感和控制模块的车辆模型,但只专注于模拟小镇环境中的路面和交通,对街道布局,城市规划和建筑结构的真实性关注较少 
2. MetaDrive:通过使用 Panda3D 和 Bullet,在视觉质量和效率之间取得平衡,提供了一个轻量级的驾驶模拟器,支持对车辆通用强化学习算法的研究 
3. NuScenes:提供了一个在真实道路上使用摄像头、雷达和激光雷达捕获的感知数据集 

2和3被设计为自动驾驶的模拟器和数据集,不太适合更广泛的城市具身任务研究 
**总结**:当前关于自动驾驶的研究平台不适用于做EmbodiedAI研究 

基于真实城市的EmbodiedAI平台:
1. CityNav:开发了一个基于 WebGL 的模拟器,该模拟器以真实城市为基础,但该平台支持的任务非常有限,导航任务中的动作是离散的,与现实世界中的导航存在很大差距 
2. V-IRL & TOUCHDOWN:都是基于 Google 地图构建,提供了最高的视觉真实感,但都仅支持离散运动 
3. AVDN:为代理提供来自卫星图像的 RGB 感知输入,这些输入精度较低 

**总结**:当前基于真实城市的平台难以满足具身智能体对高质量,高精度和连续感知与运动的要求 

基于虚构城市的EmbodiedAI平台:
1. MetaUrban:街景生成模拟器,该模拟器便于自上而下的道路布局,物体放置和动态城市交通生成设计,为地面机器人导航模拟提供了便利.但它仅关注街区级别,忽略了城市规模的设计,从而限制了智能体的活动空间.同时对象建模比较粗糙,处于贴纸级别,不太适用于图像识别算法等应用 
2. GRUtopia:为不同类型的陆地机器人提供各种导航,对话和操作任务,这些任务适用于医院,餐厅和图书馆等多种室内场景,缺陷就是仍然是室内环境 
3. AerialVLN:主要针对城市环境中无人机的视觉语言导航问题 

**总结**:这些基准主要构建在虚构的城市景观中,并且与真实的城市场景相比有所简化 

本论文研究成果有点:
1. 基于UE5构建的高度逼真的大型城市模型,模拟的高保真度 
2. 支持无人机和地面机器人的各种类型的传感和控制,为五种类型的具身任务提供数据集 
3. 高质量 3D 真实环境平台,支持各种代理,连续决策以及针对具身智能的系统基准任务 

# 3.基准平台
平台核心: 
![alt text](/assets/EmbodiedCity/figure.caption.1.jpg) 
基于此环境,建立了接口,使代理能够部署在环境中,读取第一视角观察结果的输入,并做出决策 
工作流程:
![alt text](../assets/EmbodiedCity/figure.caption.3.jpg) 

## 3.1 三维环境 
场景:北京2.8km*2.4km区域,包含建筑物,街道和其他户外元素,商业区+住宅区,支持室内室外任务 
平台:UE5.32 
构成环境的关键要素:
* 建筑物:通过使用百度地图4 和高德地图5 的街景服务,使用 Blender3 手动创建建筑物的 3D 模型.城市级细节包括约 200 座建筑物,涵盖办公大楼,购物中心、住宅区和公共设施等多种类型,且所有模型都经过纹理化和细节处理增强模拟的真实感 
* 街道:该城市共有 100 条街道,总长度约为 50 公里.街道模型包含所有必要的组件.例如车道.交叉路口.交通信号灯和道路标识。街道模型中还包括人行道,自行车道和停车区.来自**交通监控系统**和**地图服务的数据**有助于确保街道布局和交通流量模式的准确性和真实性 
* 车辆和行人:对车辆和行人等动态元素进行仿真,使其在环境中逼真地移动,仿真算法基于[Mirage 仿真系统](https://doi.org/10.1145/3557915.3560950),提供逼真的交互和行为,模拟现实世界的交通和行人动态  
* 其他元素:街道家具(长凳、路灯、标志),植被(树木、灌木、草坪)和城市便利设施(公交车站,地铁站入口,公共厕所).使用Blender创建了超过6k个城市元素.
* 城市组件展示:
![alt text](/assets/EmbodiedCity/figure.caption.4.jpg) 

该仿真是为城市规划,分析和自动驾驶车辆模拟而设计的工具,与其他模拟器相比,优势有:
*  更强大的功能:高分辨率 3D 模型和实时数据集成 
*  模拟器的定制选项允许用户模拟各种场景并探索各种城市元素,从详细的建筑特征到具体的街道级细节 
* 无人机和无人驾驶车辆的模拟 

## 3.2 接口 
利用虚幻引擎的环境,基于AirSim6开发了输入/输出接口,提供第一人称视角,控制动作包括运动,速度,加速度等 
> 这里我没理解AirSim6是怎么回事 

* 观察:
    * 无人机:图像数据,深度图像,分割图像, IMU 数据,GPS 数据,LiDAR 数据等传感器数据.位置,速度,姿态等状态信息 
    > IMU数据指用于测量加速度和角速度的加速度计和陀螺仪数据 
    > LiDAR 数据指环境的 3D 点云信息 

    * 车辆:图像数据和传感器数据.车辆的状态信息还包括位置,速度,姿态和轮角 
* 行动: 
    * 无人机:运动控制包括设置目标位置 ,目标速度,和目标方向.相机控制允许进行视角调整,其他控制包括启动或停止无人机的飞行. \
    * 车辆:驾驶控制包括转向角,加速,制动,和换挡(在档位之间切换：前进、倒退、空挡).车辆的相机控制也允许进行视角调整,其他控制包括启动或停止车辆的移动 

## 3.3 SDK和在线访问 
为了使模拟器更易于使用,在 AirSim 接口之上开发了一个基于 HTTP 协议的 Python 客户端软件开发工具包 (SDK) 和 Python 代理服务器.Python 代理服务器用于将客户端请求转换为 AirSim 接口调用并返回 AirSim 响应.借助这个 Python 代理服务器,屏蔽了客户端开发对 AirSim 使用的过时,非标准事件循环异步模块的依赖,并允许使用各种基于 HTTP 的基础设施更轻松地进行远程访问.HTTP协议主要采用 JSON 格式进行数据传输,并发送图像,相关细节可以在开源代码库中找到.该 Python 客户端 SDK 基于标准异步机制实现了同步和异步方法,以支持用户编写高并发程序,例如使用模拟器对大型模型进行并发请求. 
基于上述开放式传输协议和 Python 客户端 SDK,本文构建了一个供用户试用的在线平台.该平台支持同时模拟和控制多达 8 个代理.用户可以获取一个或多个空闲代理的控制权,并通过键盘按键,Web GUI 甚至在线 Python 代码编辑器来操控它们的移动.用户还可以通过实时视频流观看所有代理的第一人称视角.该平台开放注册和使用. 

# 4.开放城市环境中具身人工智能的基准任务
利用构建的模拟器平台,本文构建了一个包含 87.1 万个案例的数据集,该数据集包含有关其收集方式,地面真实值获取方式,案例数量以及数据集文本部分的标记数量的详细信息如下所示: 
![alt text](/assets/EmbodiedCity/table.caption.9.jpg) 
该数据集包含五个重要任务,涵盖了具身智能能力的各个方面.
开放世界中的智能体应当具备三种类似人类的能力:感知,推理和决策,测试方式如下:
* 感知:具身第一视角场景理解的任务 
* 推理:具身问答和对话的任务 
* 决策:具身行动(视觉-语言导航)和具身任务规划的任务 

下图为城市环境中五个具体任务的示意: 
![alt text](/assets/EmbodiedCity/figure.caption.6.jpg) 
1. 第一人称场景理解:第一人称场景理解要求智能体理解其周围环境并给出准确的描述,这可以被认为是进一步任务的基本能力.在**场景测试**中,从同一位置的不同视角观察,生成一组 **RGB 图像**,即场景理解的输入,输出是给定场景图像的 **文本描述** 
2. 问答:智能体可以通过自然语言进行进一步的提问.论文设计了三种类型的实体问答任务:距离,位置和计数(如下图所示).**距离**问题涉及确定智能体与周围城市元素的相对距离,比如*蓝色建筑离我更近，还是红色建筑离我更近?*,*我前面那栋建筑大约距离我多少米?*.**位置**问题评估环境中空间的关系,比如*物体 A 在物体 B 的左边还是右边?*,*哪条街在前面，A 街还是 B 街?*,**计数问题**评估智能体在环境中的准确感知能力,比如*视野范围内可以看到多少个十字路口*.在**问答测试**中,输入包括**第一视角 RGB 图像**和**关于环境的查询**,输出则是**对问题的直接文本响应** 
下图为任务示例,基于距离,位置和计数的基准来评估智能体: 
![alt text](/assets/EmbodiedCity/figure.caption.7.jpg) 
3. 对话:对话比问答更加注重智能体的复杂推理能力.对话涉及交互,所以需要维护上下文,理解对话流程.在**对话测试中**输入为**智能体的观察**与**多轮查询**,输出是**多轮响应** 
4. 行动(导航):智能体的行动主要关注使智能体能够根据自然语言指令在环境中导航,一般被称为视觉语言导航-Vision-and-Language Navigation (VLN).**行动(导航)测试**中输入是引导智能体穿越复杂的环境的**视觉感知**和**自然语言**,输出是**遵循语言指令的动作序列组成**. 
下图是具身 VLN 任务的示意图: 
![alt text](/assets/EmbodiedCity/figure_7.jpg) 
5. 任务规划:大多数情况下,真实世界中的决策并没有明确的指令,或只有不明确的任务目标.因此,对于智能体而言能够及那个复杂且长期的任务目标分解成多个子任务至关重要,本文将其称为**任务规划**.**任务规划**的输入是**第一视角的观察结果**和**给定的自然语言描述的任务目标**,输出是**智能体计划执行的一系列子任务**. 

# 5.大语言模型评估 
**Task 1** 第一人称场景理解:Claude 3最佳,且参数大的模型始终优于参数小的模型.具体结果如下: 
![alt text](/assets/EmbodiedCity/table.caption.10.jpg) 
**Task 2** 问答:GPT-4 Turbo最佳,小模型结果非常不稳定,其中不乏得分为0的情况.具体结果如下: 
![alt text](/assets/EmbodiedCity/table.caption.11.jpg) 
**Task 3** 对话:GPT-4 Turbo最佳,Qwen-VL(大规模视觉语言模型)表现不佳.具体结果如下: 
![alt text](/assets/EmbodiedCity/table.caption.12.jpg) 

**Task 4** 行动导航:
* GPT-4o 和 Claude 3 在 SR 和 SPL 上取得了最佳性能,且GPT-4o 的 NE 最低,说明 GPT-4o 具有最强的空间推理能力,始终能够将无人机引导到正确的方向. 
* 中文 LLM(Qwen-VL)在与英文 LLM 对比时表现出显著的性能下降.在 SR 和 SPL 指标上,Qwen-VL 分别比表现最佳的模型低 12% 和 15%。这一结果可归因于英文 LLM 在理解英文任务描述和将其应用于行动推理方面的优异表现.
> 这一点我存疑,应该还是大模型训练的问题 

* 所有模型在短导航任务上的表现都优于长导航任务,长导航任务涉及更长的推理链和更剧烈的场景变化,导致更高的失败率. 
* 具体结果如下图:
![alt text](/assets/EmbodiedCity/table.caption.13.jpg) 

**Task 5** 任务规划:Claude-3 在具身任务规划方面取得了最佳性能.实际上,任务规划更多地依赖于常识和上下文信息的决策能力,因此它对多模态理解能力的关注较少.同样的,规模较小的LLM表现较差.具体结果如下: 
![alt text](/assets/EmbodiedCity/table.caption.14.jpg) 

# 6.关于基准测试的讨论和局限性 
EmbodiedAI 的应用和推广:该基准不仅可以作为大型语言模型或 LLM 代理的纯粹评估,还可以作为一种 sim2real 工具,支持在部署到真实世界城市环境之前进行预训练或预测试 
引入人工:通过人工细化大模型的答案质量,过滤掉低质量的响应或修改大型语言模型提供的错误答案 
任务扩展: 
1. 多智能体协作:引入需要多个智能体之间协调和沟通才能实现共同目标的任务 
2. 人机交互:创建人类用户与代理交互的场景需要对人类行为和自然语言有更深入的理解 
3. 适应性和学习:实施测试代理从环境中学习并适应新情况和不可预见场景的能力的任务 

# 7.结论和未来工作 



